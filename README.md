<<<<<<< HEAD
# MultiSpeaker Tacotron2 in Persian Language
This repository implements [Transfer Learning from Speaker Verification to
Multispeaker Text-To-Speech Synthesis](https://arxiv.org/pdf/1806.04558.pdf) (SV2TTS) for the Persian language. The core codebase is derived from [this repository](https://github.com/Adibian/Persian-MultiSpeaker-Tacotron2), which has been updated to address deprecated features and complete setup for Persian language compatibility. The original codebase, sourced from [this repository](https://github.com/CorentinJ/Real-Time-Voice-Cloning/tree/master), has been modified to support Persian language requirements.
=======
## MultiSpeaker Tacotron2 for Persian Language

This repository contains a Persian language adaptation of [Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis (SV2TTS)](https://arxiv.org/pdf/1806.04558.pdf). The core implementation is based on [this repository](https://github.com/CorentinJ/Real-Time-Voice-Cloning/tree/master), modified to work with Persian text and phoneme data.
>>>>>>> 7c22fb2af8744a834e0e8ce31bc2701ea0eab59d

<img src="https://github.com/majidAdibian77/persian-SV2TTS/blob/master/results/model.JPG" width="800"> 

---

## Quickstart
<<<<<<< HEAD
**1. Character-set definition:**

Open the `synthesizer/persian_utils/symbols.py` file and update the `_characters` variable to include all the characters that exist in your text files. Most of Persian characters and symbols are already included in this variable as follows:
```
_characters = "ءابتثجحخدذرزسشصضطظعغفقلمنهويِپچژکگیآۀأؤإئًَُّ!(),-.:;?  ̠،…؛؟‌٪#ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz_–@+/\u200c"
```

**2. Data structures:**
=======

### Data Structure

Organize your data as follows:
>>>>>>> 7c22fb2af8744a834e0e8ce31bc2701ea0eab59d
```
dataset/persian_date/
    train_data/
        speaker1/book-1/
            sample1.txt
            sample1.wav
            ...
        ...
    test_data/
        ...
```

<<<<<<< HEAD
**3. Preprocessing:**
=======
### Preprocessing

1. **Audio Preprocessing**  
>>>>>>> 7c22fb2af8744a834e0e8ce31bc2701ea0eab59d
```
python synthesizer_preprocess_audio.py dataset --datasets_name persian_data --subfolders train_data --no_alignments
```
2. **Embedding Preprocessing**  
```
python synthesizer_preprocess_embeds.py dataset/SV2TTS/synthesizer
```

<<<<<<< HEAD
**4. Train synthesizer:**
=======
### Train the Synthesizer

To begin training the synthesizer model:
>>>>>>> 7c22fb2af8744a834e0e8ce31bc2701ea0eab59d
```
python synthesizer_train.py my_run dataset/SV2TTS/synthesizer
```

<<<<<<< HEAD
**5. Inference:**

For synthesizing wav file you must put all final models in `saved_models/final_models` directory.
If you do not train speaker encoder and vocoder models you can use pretrained models in `saved_models/default`.
=======
---
>>>>>>> 7c22fb2af8744a834e0e8ce31bc2701ea0eab59d

## Inference

To generate a wav file, place all trained models in the `saved_models/final_models` directory. If you haven’t trained the speaker encoder or vocoder models, you can use pretrained models from `saved_models/default`.

### Using WavRNN as Vocoder

```
python inference.py --vocoder "WavRNN" --text "یک نمونه از خروجی" --ref_wav_path "/path/to/sample/reference.wav" --test_name "test1"
```

### Using HiFiGAN as Vocoder (Recommended)
WavRNN is an old vocoder and if you want to use HiFiGAN you must first download a pretrained model in English.
1. **Install Parallel WaveGAN**  
```
pip install parallel_wavegan
```
2. **Download Pretrained HiFiGAN Model**  
```
from parallel_wavegan.utils import download_pretrained_model
download_pretrained_model("vctk_hifigan.v1", "saved_models/final_models/vocoder_HiFiGAN")
```
3. **Run Inference with HiFiGAN**
```
python inference.py --vocoder "HiFiGAN" --text "یک نمونه از خروجی" --ref_wav_path "/path/to/sample/reference.wav" --test_name "test1"
```
## Output Samples

You can find output samples synthesized by the trained model from this study (link to be updated) in [this directory](https://github.com/MahtaFetrat/Persian-MultiSpeaker-Tacotron2/tree/master/results/output_samples) along with the same utterances generated by two baseline models, the natural utterances, and utterances with gold spectrograms where the waveform is generated by the vocoder used in the study.

## References:
- [Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis](https://arxiv.org/pdf/1806.04558.pdf) Ye Jia, *et al*.,
- [Real-Time-Voice-Cloning repository](https://github.com/CorentinJ/Real-Time-Voice-Cloning/tree/master),
- [ParallelWaveGAN repository](https://github.com/kan-bayashi/ParallelWaveGAN)
- [Persian-MultiSpeaker-Tacotron2](https://github.com/Adibian/Persian-MultiSpeaker-Tacotron2)

## License  
This project is based on [Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning),  
which is licensed under the MIT License.  
The modifications for Persian language support are © 2025 Majid Adibian.  

  
